EventTemplate
"Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /root/hadoop\-http\-auth\-signature\-secret"""
Adding task set .* with .* tasks resource profile .*
Registering class org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager\$NodeEventDispatcher
waiting: Set\(ResultStage .*
Auth successful for .* \(auth:SIMPLE\) from .*
Initialized nodemanager with : physical\-memory=.* virtual\-memory=.* virtual\-cores=.*
Using traffic control bandwidth handler
Removed .* on .* in memory \(size: .* free: .* MiB\)
Container .* transitioned from LOCALIZING to SCHEDULED
Starting DataNode with maxLockedMemory = .*
Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService\$LocalizerTracker
"Failed to init hostsReader, disabling java.io.FileNotFoundException: .* \(No such file or directory\) at java.io.FileInputStream.open0\(Native Method\) at java.io.FileInputStream.open\(FileInputStream.java:.*\) at java.io.FileInputStream.<init>\(FileInputStream.java:.*\) at java.io.FileInputStream.<init>\(FileInputStream.java:.*\) at org.apache.hadoop.yarn.LocalConfigurationProvider.getConfigurationInputStream\(LocalConfigurationProvider.java:.*\) at org.apache.hadoop.yarn.server.resourcemanager.NodesListManager.createHostsFileReader\(NodesListManager.java:.*\) at org.apache.hadoop.yarn.server.resourcemanager.NodesListManager.serviceInit\(NodesListManager.java:.*\) at org.apache.hadoop.service.AbstractService.init\(AbstractService.java:.*\) at org.apache.hadoop.service.CompositeService.serviceInit\(CompositeService.java:.*\) at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager\$RMActiveServices.serviceInit\(ResourceManager.java:.*\) at org.apache.hadoop.service.AbstractService.init\(AbstractService.java:.*\) at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.createAndInitActiveServices\(ResourceManager.java:.*\) at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceInit\(ResourceManager.java:.*\) at org.apache.hadoop.service.AbstractService.init\(AbstractService.java:.*\) at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main\(ResourceManager.java:.*\)"""
"enqueue full packet seqno: .* offsetInBlock: .* lastPacketInBlock: false lastByteOffsetInBlock: .* src=.*, bytesCurBlock=.*, blockSize=.*, appendChunk=.*, .*"""
Opened .* server at .*
TimelineServicePublisher is not configured
Strict memory control enabled: true
"Level for block .* is StorageLevel\(memory, deserialized, .* replicas\)"""
remainingBlocks : Set .*
Now scanning bpid .* on volume .*
.* Application Attempt .* to scheduler from user root in queue root.default
Registered FSDatasetState MBean
.* filter authentication \(class=.*\) to context logs
Using Scheduler: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler
removing .* from stageTCMP
createNameNode .*
"Failed to place enough replicas: expected size is .* but only .* storage types can .* selected \(replication=.*, .* unavailable=\[DISK\], removed=\[DISK\], policy=.*{HOT:.*, storageTypes=\[DISK\], .* replicationFallbacks=\[ARCHIVE\]}\)"""
the rolling interval seconds for the NodeManager Cached Log aggregation status is .*
Preparing resources for our AM container
"No SessionScavenger set, using defaults"""
Moving to RACK_LOCAL after waiting for .*
Got job .* \(count at SparkTC.scala:.*\) with .* output partitions
Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
"dfsUsed file missing in .* will proceed with Du for space computation calculation,"""
Told master about block .*
Sent response: .* to slave0:.*
.* info for app: .*
Block .* was not found
"Using .* threads to upgrade data directories \(dfs.datanode.parallel.volumes.load.threads.num=.*, dataDirs=.*\)"""
"Initialized workflow priority mappings, override: false"""
Missing parents : List .*
stageTCMP: .* \-> .*
".* datanodeUuid=.*, infoPort=.*, infoSecurePort=.*, ipcPort=.*, storageInfo=lv=.*;cid=.*;nsid=.*;.*=.*\) Starting thread to transfer .* to .*"""
.* NodeSortingService=.*
Got the map output locations
.* Container Transitioned from ALLOCATED to ACQUIRED
Registering signal handler for .*
.* filter .* \(class=.*\) to context .*
Formatting block pool .* directory .*
Finished create editlog file .*
Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerSchedulerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerScheduler
Initialized BlockManager: .* None\)
Detailed lock hold time metrics enabled: false
"Block .* stored as bytes in memory \(estimated size .* KiB, free .* MiB\)"""
Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
The block deletion will start around .*
Scheduling .* check for .*
Clients should use master:.* to access this namenode/service.
"Accepted application .* from user: root, in queue: default"""
Saving image file .* using no compression
Starting job: count at SparkTC.scala:.*
Fetching map output statuses for shuffle .* took .* ms
The datanode lock is .* read write lock
Block .* is unknown by block manager master
Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl
Starting services required for active state
ContainerManager bound to .*
Container .* transitioned from NEW to LOCALIZING
Merging stage rdd profiles: Set\(\)
Listening HTTP traffic on .*
"parentName : , name : .* , runningTasks : .*"
"dfs.block.invalidate.limit: configured=.*, counted=.*, effected=.*"""
"Will request .* executor container\(s\) for ResourceProfile Id: .* each with .* core\(s\) and .* memory. with custom resources: <memory:.*, vCores:.*>"""
Analyzing storage directories for bpid .*
remainingBlocks: Set\(\)
Getting .* KiB\) non\-empty blocks including .* .* local and .* host\-local and .* push\-merged\-local and .* remote blocks
Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl\$ApplicationEventDispatcher
"stage=.*, .*"""
Launching task .* on executor id: .* hostname: .*
Priority ' .* ' is acceptable in queue : default for application : .*
Initialized block scanner with targetBytesPerSec .*
"Start request for .* by user .* with resource <memory:.*, vCores:.*>"""
Rolling master\-key for container\-tokens
Killing all running tasks in stage .* Stage finished
Total time to scan all replicas for block pool .*
Found block .* locally
Allocation proposal accepted
Adding .* to application .*
Adding replicas to map for block pool .* on volume .*
sun.misc.Unsafe: available
Launching container .* on host .* for executor with ID .* for ResourceProfile Id .*
Successfully created connection to .* after .* ms .* ms spent in bootstraps\)
Successfully loaded .* inodes
Created localizer for .*
Number of requests in flight .*
"update the launch time for applicationId : .* , attemptId : .* : .*"
Created Certificate for OU=.*
Call: getApplicationReport took .*
Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl\$LocalizationEventHandlerWrapper
Using ResourceCalculatorProcessTree: null
"Registered with ResourceManager as .* with total resource of <memory:.*, vCores:.*>"""
"Need to save fs image\? false \(staleImage=.*, haEnabled=.*, isRollingUpgrade=.*\)"""
Writing credentials to the nmPrivate file .*
ResultStage .* \(count at SparkTC.scala:.*\) finished in .* s
Registering class org.apache.hadoop.yarn.server.resourcemanager.NodesListManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.NodesListManager
Start MarkedDeleteBlockScrubber thread
Initializing AMS Processing chain. Root Processor=\[org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor\].
Block pool <registering> \(Datanode Uuid unassigned\) service to .* starting to offer service
Registering GenericEventTypeMetrics
Receiving .* src: .* dest: .*
"maximum\-am\-resource\-percent is insufficient to start .* single application in queue for user, it is likely set too low. skipping enforcement to allow at least one application to start"""
java.nio.Buffer.address: available
Block .* stored as bytes in memory \(estimated size .* free .* MiB\)
Changing .* acls groups to:
".* node .* clusterResource: <memory:.*, vCores:.*>"""
Jetty bound to port .*
Using ResourceCalculatorPlugin : .*
"Starting expired delegation token remover thread, tokenRemoverScanInterval=.* min\(s\)"""
Starting CacheReplicationMonitor with interval .* milliseconds
"Registering block manager .* with .* MiB RAM, .* None\)"""
Changing .* acls to: root
Adding .* new node: .*
Created store directory .*
initialized with .* entries .* lookups
Got event CONTAINER_INIT for appId .*
Don't have map outputs for shuffle .* fetching them
Connecting to ResourceManager at .*
Server created on .*
Collected remote fetch requests for .* None\) in .* ms
\-Dio.netty.noUnsafe: false
.* no suitable block pools found to scan. Waiting .* ms.
RPC server is binding to master:.*
"Missing location for the node health check script "" .* "" ."
Scanning block pool .* on volume .*
Adding filter to .* org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
Localizer started on port .*
"Resource profile .* doesn't exist, adding it"""
Updating node address : .*
org.apache.hadoop.yarn.security.ConfiguredYarnAuthorizer is instantiated.
ShuffleMapStage .* \(.* at SparkTC.scala:.*\) finished in .* s
Verifying our application has not requested more than the maximum memory capability of the cluster .* per container\)
Registering .*
SkipList is disabled
.* ResourceProfile id: .*
Started .* remote fetches in .* ms
IPC Client .* connection to .* from .*
"WriteChunk allocating new packet seqno=.*, src=.*, packetSize=.*, chunksPerPacket=.*, bytesCurBlock=.*, output stream=.*:blk_.*_.*"""
Total time to .* all replicas to map for block pool .*
"Default ResourceProfile created, executor resources: Map\(cores \-> name: cores, amount: .* script: , vendor: , memory \-> name: memory, amount: .* script: , vendor: , offHeap \-> name: offHeap, amount: .* script: , vendor: \), task resources: Map\(cpus \-> name: cpus, amount: .*"""
"registered UNIX signal handlers for \[TERM, HUP, INT\]"""
Successfully started service 'sparkDriver' on port 35469.
Starting the user application in .* separate Thread
Allocated new applicationId: .*
Number of blocks under construction: .*
.* broadcast .*
Registering class org.apache.hadoop.yarn.nodelabels.event.NodeLabelsStoreEventType for class org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager\$ForwardingEventHandler
"maximum\-am\-resource\-percent is insufficient to start .* single application in queue, it is likely set too low. skipping enforcement to allow at least one application to start"""
ContainerTokenKeyRollingInterval: .* and ContainerTokenKeyActivationDelay: .*
Getting .* block .*
"Failed to place enough replicas, still in need of .* to reach .* \(unavailableStorages=\[DISK\], storagePolicy=.*{HOT:.*, storageTypes=\[DISK\], .* replicationFallbacks=\[ARCHIVE\]}, newBlock=.*\) All required storage types are unavailable: unavailableStorages=\[DISK\], storagePolicy=.*{HOT:.*, storageTypes=\[DISK\], .* replicationFallbacks=\[ARCHIVE\]}"""
.* max memory .* = .*
Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> \(Datanode Uuid unassigned\) service to .*
"Removed TaskSet .* whose tasks have all completed, from pool"
node0 Scavenging every .*
Running task .* in stage .* \(TID .*
Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port .*
Job .* is finished. Cancelling potential speculative or zombie tasks for this job
Registered .* MBean
"assignedContainer application attempt=.* container=.* queue=.* clusterResource=<memory:.*, vCores:.*> type=.* requestedPartition="""
\-Djava.net.preferIPv4Stack: false
Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager\$ApplicationEventDispatcher
Rolling edit logs
"Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: .* scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false."""
"Starting task .* in stage .* \(TID .* executor .* partition .* .*, .* bytes\) taskResourceAssignments Map\(\)"""
Initializing cache loader: MemoryMappableBlockLoader.
NameNode RPC up at: .*
missing: List\(\)
storing master key with keyID .*
Clear node set for .*
Periodic Directory Tree Verification scan starting in .* with interval of .* and throttle limit of .*
Start fetching local blocks: .*
Generated new storageID .* for directory .*
"Job .* finished: count at SparkTC.scala:.*, took .* s"""
.* to active state
Application report for .* \(state: .*\)
Loaded image for txid .* from .*
Storing .* Certificate and Private Key
Final stage: ResultStage .* \(count at SparkTC.scala:.*\)
"disabled placement handler will .* used, all scheduling requests will .* rejected."""
Locking is disabled for .*
Disk Validator 'basic' is loaded.
.* = true
.* not found
"Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME."""
DFSClient seqno: .* reply: SUCCESS reply: SUCCESS reply: SUCCESS downstreamAckTimeNanos: .* flag: .* flag: .* flag: .*
Starting Web\-server for .* at: .*
Launching .*
.* = false
Putting block .* without replication took .* ms
Asked to send map output locations for shuffle .* to .*
"Convert map statuses for shuffle .* mappers .*, partitions .*"""
Finished write mirror .*
Waiting for spark context initialization...
"Failed to place enough replicas: expected size is .* but only .* storage types can .* selected \(replication=.*, .* unavailable=\[DISK, ARCHIVE\], removed=\[DISK\], policy=.*{HOT:.*, storageTypes=\[DISK\], .* replicationFallbacks=\[ARCHIVE\]}\)"""
Done removing broadcast .* response is .*
Initializing user root
Configured hostname is .*
Only one namespace edits storage directory \(dfs.namenode.edits.dir\) configured. Beware of data loss due to lack of redundant storage directories!
Creating .* new application reference for app .*
Application .* from user: root activated in queue: root.default
"Planning to load image: FSImageFile\(file=.*, cpktTxId=.*\)"""
"src: .* dest: .* bytes: .* op: HDFS_WRITE, cliID: .* offset: .* srvID: .* blockid: .*, duration\(ns\): .*"""
ContainerManager started at .*
Retry cache on namenode is enabled
Using ResourceCalculatorPlugin: .*
Uploading resource file : .* -> hdfs : .*
Rejecting .* fsimage due to small time delta and txnid delta. Time since previous checkpoint is .* expecting at least .* txnid delta since previous checkpoint is .* expecting at least .*
.* finished scanning block pool .*
NNTop conf: .* = .*
Generated and persisted new Datanode UUID .*
Roll Edit Log from .*
Only one image storage directory \(dfs.namenode.name.dir\) configured. Beware of data loss due to lack of redundant storage directories!
.* BlockManager .* None\)
Unable to find ' .* ' .
supergroup = supergroup
Created broadcast .* from broadcast at DAGScheduler.scala:.*
Setting up the launch environment for our AM container
"Filter initializers set : org.apache.hadoop.http.lib.StaticUserWebFilter,org.apache.hadoop.hdfs.web.AuthFilterInitializer"""
Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncherEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher
Submitted .* unlocalized container requests.
For namenode .* using BLOCKREPORT_INTERVAL of .* CACHEREPORT_INTERVAL of .* Initial delay: .* heartBeatInterval=.*
dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
Added new volume: .*
removing broadcast .*
Refreshing hosts \(include/exclude\) list
Setting up container launch context for our AM
Successfully started service 'SparkUI' on port 42407.
POSIX ACL inheritance enabled\? true
Finished task .* in stage .* \(TID .* in .* ms on .* \(executor .*
Storage policy satisfier is disabled
jetty\-9.4.43.v20210629; built: 2021\-06\-30T11:.*:22.254Z; git: .* jvm 1.8.0_392\-8u392\-ga\-1~22.04\-b08
.* enabled\? true
Balancing bandwidth is .* bytes/s
MemoryStore started with capacity .* MiB
fsLock is fair: true
"DataTransfer, at .*: Transmitted .* \(numBytes=.*\) to .*"""
.*
Handling deprecation for .*
Started .*
"Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: .*"""
Connecting to driver: spark://CoarseGrainedScheduler@slave0:.*
Application .* \- appId: .* user: root leaf\-queue of parent: root #applications: .*
.* metrics system started
Enable NameNode state context:false
Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl\$ContainerEventDispatcher
.*: Set\(\)
Adding block pool .*
.* accumulator .*
Computing capacity for map .*
.* State change from .* to .* on event = .*
Adding new storage ID .* for DN .*
ApplicationMaster registered as NettyRpcEndpointRef\(spark://YarnAM@slave0:.*\)
BlockManagerMasterEndpoint up
Registering class org.apache.hadoop.yarn.server.resourcemanager.RMFatalEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager\$RMFatalEventDispatcher
Time to .* replicas to map for block pool .* on volume .*
Submitted application: SparkTC
Unable to load native\-hadoop library for your platform... using builtin\-java classes where applicable
Writing shuffle index file for mapId .* with length .*
Updating the current master key for generating delegation tokens
Starting JVM pause monitor
"Added .* in memory on .* \(size: .* .*, free: .* MiB\)"""
Loading .* INodes.
Application .* is submitted without priority hence considering default queue/cluster priority: .*
KeyProvider: null
Edit logging is async:true
.* Container Transitioned from NEW to ALLOCATED
AM registration .*
Adding protocol .* to the server
Block pool .* \(Datanode Uuid .* service to .* successfully registered with NN
dnUserName = root
dfs.namenode.datanode.registration.ip\-hostname\-check=.*
Created YarnClusterScheduler
Starting log segment at .*
Submitted application .*
Container .* transitioned from SCHEDULED to RUNNING
Reading piece .* of .*
Starting executor ID .* on host .*
Start fetching local blocks:
Starting resource\-monitoring for .*
Creating fetch request of .* at .* None\) with .* blocks
Setting up storage: nsid=.*;bpid=.*;lv=.*;nsInfo=lv=.*;cid=.*;nsid=.*;.*=.*;bpid=.*;dnuuid=.*
Started reading broadcast variable .* with .* pieces \(estimated total size .* MiB\)
Initialized queue: .*
found resource .* at .*
Number of transactions : .* Total time for transactions .* : .* Number of transactions batched in Syncs : .* Number of syncs : .* SyncTimes .* : .*
IPC Server Responder: starting
"Initialized CapacityScheduler with calculator=.* org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator, minimumAllocation=<<memory:.*, vCores:.*>>, maximumAllocation=<<memory:.*, vCores:.*>>, asynchronousScheduling=.*, asyncScheduleInterval=.*, assignMultipleEnabled=.*, maxAssignPerHeartbeat=.*, offswitchPerHeartbeatLimit=.*"""
Ending log segment .*
Registering class org.apache.hadoop.yarn.server.nodemanager.ContainerManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl
Log Size Trigger :.* txns
"Initialized queue mappings, override: false"""
"Rolling master\-key for container\-tokens, got key with id .*"""
.* volume \- .* StorageType: DISK
Registering RDD .* \(.* at SparkTC.scala:.*\) as input to shuffle .*
Http request log for .* is not defined
Got null for restCsrfPreventionFilter \- will not do any filtering.
"PacketResponder: .*, type=.* terminating"""
Container .* is localizing: .*
ApplicationAttemptId: .*
Elastic memory control enabled: false
Reading broadcast variable .* took .* ms
Recovering unfinalized segments in .*
Instantiating NMWebApp at .*
.* Container Transitioned from ACQUIRED to RUNNING
Parents of final stage: List\(ShuffleMapStage .*
Starting Socket Reader #1 for port .*
Node ID assigned is : .*
"MultiNode scheduling is 'false', and configured policies are"""
Finished task .* in stage .* \(TID .* bytes result sent to driver
"Not enough replicas was chosen. Reason: {NODE_TOO_BUSY=.*, NO_REQUIRED_STORAGE_TYPE=.*}"""
Logging initialized .* to org.eclipse.jetty.util.log.Slf4jLog
Task .* release .* from .*
running: Set\(ShuffleMapStage .*
No Resource plugins found from configuration!
dfs.datanode.directoryscan.throttle.limit.ms.per.sec set to value above .* ms/sec. Assuming default value of .*
AMRMTokenKeyRollingInterval: .* and AMRMTokenKeyActivationDelay: .* ms
Registering app attempt : .*
"Initialized root queue root: numChildQueue= .* capacity=.*, absoluteCapacity=.*, usedResources=<memory:.*, vCores:.*>usedCapacity=.*, numApps=.*, numContainers=.*"""
Running Spark version 3.3.2
Limiting resource is cpus at .* tasks per executor
Updating epoch to .* and clearing cache
"Received .* containers from YARN, launching executors on .* of them."""
Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.event.LogHandlerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler
Updating AMRMToken
Initializing quota with .* thread\(s\)
"Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans."""
Registering class org.apache.hadoop.yarn.server.resourcemanager.RMAppManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.RMAppManager
Registering class org.apache.hadoop.yarn.server.resourcemanager.nodelabels.NodeAttributesStoreEventType for class org.apache.hadoop.yarn.server.resourcemanager.nodelabels.NodeAttributesManagerImpl\$ForwardingEventHandler
"The pluggable device framework is not enabled. If you want, please set true to yarn.nodemanager.pluggable\-device\-framework.enabled"""
Application with id .* submitted by user root
ShuffleMapTask finished on .*
"computePacketChunkSize: src=.*, chunkSize=.*, chunksPerPacket=.*, packetSize=.*"""
Got assigned task .*
Initialized the Default Decommission and Maintenance monitor
dfs.namenode.startup.delay.block.deletion.sec is set to .*
Successfully registered with driver
Not .* recoverable state store. Nothing to recover.
"Failed to place enough replicas, still in need of .* to reach .* \(unavailableStorages=\[DISK, ARCHIVE\], storagePolicy=.*{HOT:.*, storageTypes=\[DISK\], .* replicationFallbacks=\[ARCHIVE\]}, newBlock=.*\) All required storage types are unavailable: unavailableStorages=\[DISK, ARCHIVE\], storagePolicy=.*{HOT:.*, storageTypes=\[DISK\], .* replicationFallbacks=\[ARCHIVE\]}"""
.* memory check enabled: true
Scheduled Metric snapshot period at .* second\(s\).
Got cleaning task .*
"Quota initialization completed in .* milliseconds name space=.* storage space=.* storage types=RAM_DISK=.*, SSD=.*, DISK=.*, ARCHIVE=.*, PROVIDED=.*"""
Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadService
looking for newly runnable stages
Application .* transitioned from INITING to RUNNING
.* Node Transitioned from NEW to RUNNING
Registering class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncherEventType for class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher
initializing replication queues
"Completed update blocks map and name cache, total waiting duration .*"""
Epoch for TaskSet .*
"Assigned container .* of capacity <memory:.*, vCores:.*> on host .*, which has .* containers, <memory:.*, vCores:.*> used and <memory:.*, vCores:.*> available after allocation"""
Starting BPOfferServices for nameservices: <default>
Loaded FSImage in .* seconds.
per directory file limit = .*
"Will allocate AM container, with .* memory including .* overhead"""
"Initialized parent\-queue root name=.*, fullname=.*"""
SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: .*
NMTokenKeyRollingInterval: .* and NMTokenKeyActivationDelay: .*
Storage directory with location .* file : .* is not formatted for namespace 654690358. Formatting...
waiting: Set\(ShuffleMapStage .* ResultStage .*
Application lifelime monitor interval set to .* ms.
Application .* transitioned from NEW to INITING
Time taken to scan block pool .* on .*
SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set\(root\); groups with view permissions: Set\(\); users with modify permissions: Set\(root\); groups with modify permissions: Set\(\)
Uploading resource file:/spark/examples/jars/scopt_2.12\-3.7.1.jar \-> .*
Copying from .* to .*
Registering the ApplicationMaster
"Application .* \- appId: .* user: root, leaf\-queue: root.default #user\-pending\-applications: .* #user\-active\-applications: .* #queue\-pending\-applications: .* #queue\-active\-applications: .* #queue\-nonrunnable\-applications: .*"""
VM type = 64\-bit
Log Aggregation is disabled.So is the LogAggregationStatusTracker.
"Sending fileName : .* , fileSize : .* Sent total : .* bytes. Size of last segment intended to send : .* bytes."
Queued packet seqno: .* offsetInBlock: .* lastPacketInBlock: false lastByteOffsetInBlock: .*
Updating block info on master .* for .* None\)
After removal of stage .* remaining stages = .*
".* allocation = <memory:.*, vCores:.*>"""
Registered executor NettyRpcEndpointRef\(spark\-client://Executor\) .* with ID .* ResourceProfileId .*
.* = .*
Container Log Monitor Enabled: false
"assignedContainer queue=.* usedCapacity=.* absoluteUsedCapacity=.* used=<memory:.*, vCores:.*> cluster=<memory:.*, vCores:.*>"""
Registering class org.apache.hadoop.yarn.server.nodemanager.NodeManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.NodeManager
Block pool .* \(Datanode Uuid .* service to .* beginning handshake with NN
Preparing Local resources
Getting .* B\) non\-empty blocks including .* B\) local and .* host\-local and .* push\-merged\-local and .* B\) remote blocks
Refresh request received for nameservices: null
Doing the fetch; tracker endpoint = NettyRpcEndpointRef\(spark://MapOutputTracker@slave0:.*\)
Getting local shuffle block .*
Loaded properties from hadoop\-metrics2.properties
Scheduled health check for volume .*
"Failed to place enough replicas, still in need of .* to reach .* storagePolicy=.*{HOT:.*, storageTypes=\[DISK\], .* replicationFallbacks=\[ARCHIVE\]}, newBlock=.*\) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy and org.apache.hadoop.net.NetworkTopology"""
Registered webapp guice modules
capacity = .* = .* entries
IPC Server listener on .* starting
Starting executor with user classpath \(userClassPathFirst = false\): .*
checking for deactivate of application .*
Web server init done
Block pool storage directory for location .* and block pool id .* is not formatted. Formatting ...
Finalizing edits file .* \-> .*
fs.defaultFS is .*
"Removed .* on .* in memory \(size: .* KiB, free: .* MiB\)"""
Parents of final stage: List\(\)
Cleaning indylambda closure: .*
"maxBytesInFlight : .* , targetRemoteRequestSize : .* , maxBlocksInFlightPerAddress : .*"
Storing RMDTMasterKey.
Skipping monitoring container .* since CPU usage is not yet available.
Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=.*}
"Nodemanager resources is set to: <memory:.*, vCores:.*>"""
Rolling master\-key for nm\-tokens
"Successfully sent block report .* with lease ID .* to namenode: .* containing .* storage report\(s\), of which we sent 1. The reports had .* total blocks and used .* RPC\(s\). This took .* msecs to generate and .* msecs for RPC and NN processing. Got back one command: FinalizeCommand/5."""
"launchContainer: \[bash, .*"""
Web app .* started at .*
=============================================================.*
Registered DN .*
Put block .* locally took .* ms
Removing .* .*
Localizer CWD set to .* = .*
No edit log streams selected.
DefaultSessionIdManager workerName=.*
Block .* of size .* dropped from memory \(free .*
"Storing attempt: AppId: .* AttemptId: .* MasterContainer: Container: \[ContainerId: .* AllocationRequestId: .* Version: .* NodeId: slave0:.*, NodeHttpAddress: slave0:.*, Resource: <memory:.*, vCores:.*>, Priority: .* Token: Token { kind: ContainerToken, service: .* }, ExecutionType: GUARANTEED, \]"""
Replica Cache file: .* doesn't exist
.* sending packet seqno: .* offsetInBlock: .* lastPacketInBlock: false lastByteOffsetInBlock: .*
"NodeManager from node .* httpPort: .* registered with capability: <memory:.*, vCores:.*>, assigned nodeId .*"""
No tasks for locality level .* so moving to locality level .*
Got local blocks in .* ms
Unpersisting TorrentBroadcast .*
Number threads for balancing is .*
.* serial map: bits=.* maxEntries=.*
Retry cache will use .* of total heap and retry cache entry expiry time is .* millis
Getting .* KiB\) non\-empty blocks including .* .* local and .* host\-local and .* push\-merged\-local and .* KiB\) remote blocks
Sending fetch chunk request .* to .*
Sending request for .* blocks .* .* from .*
Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServicesEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices
task .* in stage .* \(TID .* epoch is .*
Got finalize command for block pool .*
YarnClusterScheduler.postStartHook done
AMRMProxyService is disabled
Caching file names occurring more than .* times
Starting container .*
ContainersMonitor enabled: true
No custom resources configured for .*
.* ip = .* and hostname = .*
Missing parents: List\(ShuffleMapStage .*
"Block .* stored as values in memory \(estimated size .* .*, free .* MiB\)"""
Found Resource plugins from configuration: null
HA Enabled: false
Registering class org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager\$SchedulerEventDispatcher
"The specific max attempts: .* for application: .* is invalid, because it is less than or equal to zero. Use the rm max attempts instead."""
Fetching outputs for shuffle .*
.* global filter 'safety' \(class=.*\)
Shuffle index for mapId .*
Received .* : .* src : .* dest : .* of size .*
Checkpoint Period :.* secs .* min\)
Creating password for .*
Storing application with id .*
Updated info of block .*
fsOwner = root \(auth:SIMPLE\)
Image file .* of size .* bytes saved in .* seconds .
Finished loading FSImage in .* msecs
Adding .* node .* to the list of included hosts from .*
Registering class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEventType for class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore\$ForwardingEventHandler
"Setting the resources allocated to containers to <memory:.*, vCores:.*>"""
Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager\$ApplicationAttemptEventDispatcher
Created local directory at .*
Lock on .* acquired by nodename .*
Submitting application .* to ResourceManager
Create AMRMToken for ApplicationAttempt: .*
